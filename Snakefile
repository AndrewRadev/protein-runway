from snakemake.io import glob_wildcards
import MDAnalysis as mda

from lib.trajectory_writer import TrajectoryWriter
from lib.normal_modes import NormalModes
from lib.segmentation_parsers import *

protein_names = glob_wildcards("01_input/traj/{protein_name}_10-20ns_100snap.trr").protein_name

MERIZO_OK   = 'vendor/merizo_ok.txt'
CHAINSAW_OK = 'vendor/chainsaw_ok.txt'

rule all:
    input:
        expand("03_output/{protein_name}.segmentation.tsv", protein_name=protein_names),
        expand("03_output/{protein_name}.nmd_traj.pdb", protein_name=protein_names),
        aggregated_csv=report("benchmarks/aggregated_runtime.csv")

rule merizo_setup:
    output: directory('vendor/merizo')
    shell:
        """
        # Using our own fork of Merizo to fix an issue with histidine residues:
        git clone https://github.com/AndrewRadev/Merizo vendor/merizo
        cd vendor/merizo

        # Check out known commit:
        git checkout d44ae81

        # Delete git history to save space:
        rm -rf .git/
        """

rule merizo_qa:
    input: 'vendor/merizo'
    output: MERIZO_OK
    shell:
        """
        python vendor/merizo/predict.py -i vendor/merizo/examples/2xdqA.pdb > {output}
        """

rule chainsaw_setup:
    output: directory('vendor/chainsaw')
    shell:
        """
        git clone https://github.com/JudeWells/Chainsaw vendor/chainsaw
        cd vendor/chainsaw

        # Check out known commit:
        git checkout 9ced6e6

        # Delete git history to save space:
        rm -rf .git/

        # Compile stride:
        cd stride
        tar -xzf stride.tgz
        make
        """

rule chainsaw_qa:
    input: 'vendor/chainsaw'
    output: CHAINSAW_OK
    shell:
        """
        python vendor/chainsaw/get_predictions.py \
            --structure_file vendor/chainsaw/example_files/AF-A0A1W2PQ64-F1-model_v4.pdb \
            --output {output}
        """

rule get_chainsaw_clustering:
    input:
        structure_file="02_intermediate/pdb/{protein_name}.static.pdb",
        chainsaw_ok=CHAINSAW_OK
    output:
        output_file="02_intermediate/chainsaw/{protein_name}.tsv",
        output_tsv="benchmarks/chainsaw_clustering/{protein_name}.tsv",
    benchmark:
        "benchmarks/chainsaw_clustering/{protein_name}.tsv"
    shell:
        """
        python vendor/chainsaw/get_predictions.py \
            --structure_file {input.structure_file} \
            --output {output.output_file}

        # Clean up after chainsaw
        if [ -d results ]; then
          rmdir results/
        fi
        """

rule get_merizo_clustering:
    input:
        structure_file="02_intermediate/pdb/{protein_name}.static.pdb",
        merizo_ok=MERIZO_OK
    output:
        output_file="02_intermediate/merizo/{protein_name}.tsv",
        output_tsv="benchmarks/merizo_clustering/{protein_name}.tsv"
    benchmark:
        "benchmarks/merizo_clustering/{protein_name}.tsv"
    shell:
        """
        # PDB Chain generated by MDAnalysis is labelled "X"
        python vendor/merizo/predict.py \
            --pdb_chain X \
            -i {input.structure_file} \
            > {output.output_file}
        """

rule build_pdbs:
    input:
        topology="01_input/top/{protein_name}_complex.top",
        trajectory="01_input/traj/{protein_name}_10-20ns_100snap.trr"
    output:
        traj_pdb_file="02_intermediate/pdb/{protein_name}.with_traj.pdb",
        traj_ca_pdb_file="02_intermediate/pdb/{protein_name}.with_traj.ca.pdb",
        traj_ca_dcd_file="02_intermediate/pdb/{protein_name}.with_traj.ca.dcd",
        static_pdb_file="02_intermediate/pdb/{protein_name}.static.pdb",
        output_tsv="benchmarks/build_pdbs/{protein_name}.tsv"
    benchmark:
        "benchmarks/build_pdbs/{protein_name}.tsv"
    run:
        u = mda.Universe(input.topology, input.trajectory)
        writer = TrajectoryWriter(u)

        writer.write_static_file(output.static_pdb_file,   'protein')
        writer.write_trajectory_file(output.traj_pdb_file, 'protein')

        writer.write_trajectory_file(output.traj_ca_pdb_file, 'protein and name is CA')
        writer.write_trajectory_file(output.traj_ca_dcd_file, 'protein and name is CA')

rule build_nmd_file:
    input:
        pdb_file="02_intermediate/pdb/{protein_name}.with_traj.pdb",
    output:
        nmd_file="02_intermediate/pca/{protein_name}.nmd",
        output_tsv="benchmarks/build_nmd/{protein_name}.tsv"
    benchmark:
        "benchmarks/build_nmd/{protein_name}.tsv"
    run:
        nmd_traj = NormalModes()
        nmd_traj = nmd_traj.generate_nmd_from_pdb(input.pdb_file, output.nmd_file)

rule build_nmd_trajectory:
    input:
        nmd_file="02_intermediate/pca/{protein_name}.nmd",
    output:
        traj_file="03_output/{protein_name}.nmd_traj.pdb",
        output_tsv="benchmarks/nmd_trajectory/{protein_name}.tsv"
    benchmark:
        "benchmarks/nmd_trajectory/{protein_name}.tsv"
    run:
        #doesn't seem like good practice to refer to variables from a previous rule,
        #  but it also seems weird to me to now have two instantiated NormalModes objects
        #  that aren't connected, or perhaps that doesn't matter for the snakemake logic?
        nmd_traj = NormalModes()
        nmd_traj.parse_nmd_file(input.nmd_file)
        mda_universe = nmd_traj.generate_trajectory()

        writer = TrajectoryWriter(mda_universe)
        writer.write_trajectory_file(output.traj_file)

rule generate_amsm:
    input:
        dcd_file="02_intermediate/pdb/{protein_name}.with_traj.ca.dcd"
    output:
        amsm_path="02_intermediate/bio3d_geostas/{protein_name}_amsm.csv",
        output_tsv="benchmarks/amsm/{protein_name}.tsv"
    benchmark:
        "benchmarks/amsm/{protein_name}.tsv"
    shell:
        """
        Rscript scripts/generate_amsm.R {input.dcd_file} {output.amsm_path}
        """

rule segment_by_geostas:
    input:
        dcd_file="02_intermediate/pdb/{protein_name}.with_traj.ca.dcd",
        amsm_path="02_intermediate/bio3d_geostas/{protein_name}_amsm.csv"
    output:
        clustering=directory("02_intermediate/bio3d_geostas/{protein_name}"),
        output_tsv="benchmarks/geostas/{protein_name}.tsv"
    benchmark:
        "benchmarks/geostas/{protein_name}.tsv"
    shell:
        """
        Rscript scripts/segment_with_geostas.R {input.dcd_file} {input.amsm_path} {output.clustering}
        """

rule collect_segmentation_intermediates:
    input:
        chainsaw="02_intermediate/chainsaw/{protein_name}.tsv",
        merizo="02_intermediate/merizo/{protein_name}.tsv",
        bio3d_geostas="02_intermediate/bio3d_geostas/{protein_name}/"
    output:
        segmentation="03_output/{protein_name}.segmentation.tsv",
        output_tsv="benchmarks/segment_intermediates/{protein_name}.tsv"
    benchmark:
        "benchmarks/segment_intermediates/{protein_name}.tsv"
    run:
        #should change this to loop through all input files without manually instantiating somehow?
        chainsaw_input = ChainsawParser(input.chainsaw)
        merizo_input = MerizoParser(input.merizo)
        geostas_input = GeostasParser(input.bio3d_geostas)

        #skipping merizo for now
        write_segmentations([chainsaw_input, geostas_input], output.segmentation)

rule report_plot:
    input:
        tsv_files=expand("benchmarks/{rule_name}/{protein_name}.tsv",
                         rule_name=["build_pdbs", "nmd_trajectory", "amsm",
                                    "geostas", "segment_intermediates",
                                    "merizo_clustering",
                                    "chainsaw_clustering"],
                         protein_name=protein_names)
    output:
        aggregated_csv=report("benchmarks/aggregated_runtime.csv"),
        runtime_plot=report("benchmarks/runtime_plot.png")
    run:
        import pandas as pd

        # Using Agg to avoid problems with threading:
        # https://matplotlib.org/stable/users/explain/figure/backends.html
        #
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        from pathlib import Path

        def aggregate_benchmarks(tsv_files, output_file):
            benchmarks = []
            for file in tsv_files:
                data = pd.read_csv(file, sep="\t")
                rule_name = Path(file).parts[-2]  # Extract rule name
                sample_name = Path(file).stem  # Extract protein name
                runtime = data["cpu_time"].sum() if "cpu_time" in data.columns else 0
                benchmarks.append({"rule": rule_name, "sample": sample_name, "runtime": runtime})
            df = pd.DataFrame(benchmarks)
            df.to_csv(output_file, index=False)
            print(f"Aggregated data saved to {output_file}")

        def plot_sample_runtimes(data_file, output_plot):
            df = pd.read_csv(data_file)
            plt.figure(figsize=(12, 8))
            for rule in df["rule"].unique():
                subset = df[df["rule"] == rule]
                plt.scatter(subset["runtime"], [rule] * len(subset), label=rule, alpha=0.7)
            plt.xlabel("Runtime (seconds)")
            plt.ylabel("Rule")
            plt.title("Runtime of Snakemake Rules by Sample")
            plt.tight_layout()
            plt.savefig(output_plot)
            plt.close()
            print(f"Runtime plot saved to {output_plot}")

        aggregate_benchmarks(input.tsv_files, output.aggregated_csv)
        plot_sample_runtimes(output.aggregated_csv, output.runtime_plot)
